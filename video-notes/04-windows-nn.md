# Lecture 4: Word Window Classification Neural Networks

[Lecture 4 Stanford NLP on Classifiers and NNs](https://www.youtube.com/watch?v=uc2_iwVqrRI&t=0s&list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6&index=5)
  - [slides](http://web.stanford.edu/class/cs224n/lectures/lecture4.pdf)

# Lecture Plan
- Lecture plan: [1m23s](https://youtu.be/uc2_iwVqrRI?t=1m23s)
  - Classification background: [3:07](https://youtu.be/ASn7ExxLZws?t=3m7s)
  - Updating word vectors for classification []()
  - Window classification and cross entropy []()
  - Single layer neural network []()
  - Max-Margin los and **backprop** []()

# Highlights

## Details of the softmax
  - Softmax details: [4:02](https://youtu.be/uc2_iwVqrRI?t=4m02s)
  - Calc Y given X, eg. p(y|x)
    1. take y'th row of the W matrix  (unnormalized scores)
    2. Dot product y'th row of W with X
    
# Suggested reading
  - http://web.stanford.edu/class/cs224n/syllabus.html

# Assignment info
  - [HW#1](http://web.stanford.edu/class/cs224n/assignment1/index.html)
  - [HW#1-solutions](http://web.stanford.edu/class/cs224n/assignment1/assignment1-solution.pdf)

# References

